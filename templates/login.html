<!DOCTYPE html>
<html>

<head>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/socket.io/2.3.1/socket.io.js"></script>
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/webcamjs/1.0.25/webcam.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js"></script>
    <link rel="preconnect" href="https://fonts.gstatic.com">
    <link href="https://fonts.googleapis.com/css2?family=Poppins:wght@100;200;400;500;700;800;900&display=swap"
        rel="stylesheet">
    <script src="../scripts/camvas.js"></script>
    <script src="../scripts/pico.js"></script>
    <script src="../scripts/lploc.js"></script>

    <style>
        html,
        body {
            width: 100%;
            height: 100%;
            margin: 0;
            overflow: hidden;
        }
        
        .camdiv{
            display: flex;
            flex-direction: column;
            justify-content: center;
            min-height: 50vh;
        }

        canvas {
            border-radius: 100%;
        }

        .faceheader {
            margin-top: 0;
            text-align: center;
            font-family: poppins;
            font-size: 3.2rem;
            font-weight: 500;
        }

        .alignface{
            margin-top: 0;
            text-align: center;
            font-family: poppins;
            font-size: 1.2rem;
            font-weight: 400;
        }

        .statusmessage{
            margin-top: 0;
            text-align: center;
            font-family: poppins;
            font-size: 1rem;
            font-weight: 400;
        }
        
    </style>

</head>

<body>
    <div class="row">
        <div class="col-12 col-md-12 col-lg-12 faceheader align-middle">
            <p>Face verification</p>
        </div>
    </div>
    <div class="camdiv">
        <center>
            <canvas id="mycamera"></canvas>
        </center>
    </div>
    <div class="row">
        <div class="col-12 col-md-12 col-lg-12 alignface align-middle">
            <p>Place your face on the camera</p>
        </div>
        <div class="col-12 col-md-12 col-lg-12 statusmessage align-middle">
            <p><img src="../images/error.svg" height="24px" width="auto" /> failed! <a href="#">Click here</a> to retry</p>
        </div>
    </div>
    <script>
        let socketConnected = true;
        // var socket = io("http://192.168.1.16", {
        //     transports: ['websocket']
        // });
        // socket.on('connect', function () {
        //     console.log("Connected")
        //     socketConnected = true;
        // });
        let faceDetected = false;
        var initialized = false;

        function button_callback() {
            if (socketConnected) {
                /*
                (0) check whether we're already running face detection
                */
                if (initialized)
                    return; // if yes, then do not initialize everything again
                /*
                    (1) initialize the pico.js face detector
                */
                var update_memory = pico.instantiate_detection_memory(
                    5); // we will use the detecions of the last 5 frames
                var facefinder_classify_region = function (r, c, s, pixels, ldim) {
                    return -1.0;
                };
                var cascadeurl =
                    'https://raw.githubusercontent.com/nenadmarkus/pico/c2e81f9d23cc11d1a612fd21e4f9de0921a5d0d9/rnt/cascades/facefinder';
                fetch(cascadeurl).then(function (response) {
                    response.arrayBuffer().then(function (buffer) {
                        var bytes = new Int8Array(buffer);
                        facefinder_classify_region = pico.unpack_cascade(bytes);
                        console.log('* facefinder loaded');
                    })
                })
                /*
                    (2) initialize the lploc.js library with a pupil localizer
                */
                var do_puploc = function (r, c, s, nperturbs, pixels, nrows, ncols, ldim) {
                    return [-1.0, -1.0];
                };
                var puplocurl = 'https://drone.nenadmarkus.com/data/blog-stuff/puploc.bin'
                fetch(puplocurl).then(function (response) {
                    response.arrayBuffer().then(function (buffer) {
                        var bytes = new Int8Array(buffer);
                        do_puploc = lploc.unpack_localizer(bytes);
                        console.log('* puploc loaded');
                    })
                })
                
                /*
                    (3) get the drawing context on the canvas and define a function to transform an RGBA image to grayscale
                */

                const height = 500;
                const width = 500;

                var ctx = document.getElementsByTagName('canvas')[0].getContext('2d');
                ctx.canvas.width = width;
                ctx.canvas.height = height;

                function rgba_to_grayscale(rgba, nrows, ncols) {
                    var gray = new Uint8Array(nrows * ncols);
                    for (var r = 0; r < nrows; ++r)
                        for (var c = 0; c < ncols; ++c)
                            // gray = 0.2*red + 0.7*green + 0.1*blue
                            gray[r * ncols + c] = (2 * rgba[r * 4 * ncols + 4 * c + 0] + 7 * rgba[r * 4 * ncols + 4 *
                                    c + 1] +
                                1 *
                                rgba[r * 4 * ncols + 4 * c + 2]) / 10;
                    return gray;
                }
                /*
                    (4) this function is called each time a video frame becomes available
                */
                var processfn = function (video, dt) {
                    // render the video frame to the canvas element and extract RGBA pixel data
                    ctx.drawImage(video, 0, 0, width, height);
                    var rgba = ctx.getImageData(0, 0, width, height).data;
                    // prepare input to `run_cascade`
                    image = {
                        "pixels": rgba_to_grayscale(rgba, height, width),
                        "nrows": height,
                        "ncols": width,
                        "ldim": width
                    }
                    params = {
                        "shiftfactor": 0.1, // move the detection window by 10% of its size
                        "minsize": 100, // minimum size of a face
                        "maxsize": 1000, // maximum size of a face
                        "scalefactor": 1.1 // for multiscale processing: resize the detection window by 10% when moving to the higher scale
                    }
                    // run the cascade over the frame and cluster the obtained detections
                    // dets is an array that contains (r, c, s, q) quadruplets
                    // (representing row, column, scale and detection score)
                    dets = pico.run_cascade(image, facefinder_classify_region, params);
                    dets = update_memory(dets);
                    dets = pico.cluster_detections(dets, 0.2); // set IoU threshold to 0.2
                    // draw detections
                    for (i = 0; i < dets.length; ++i)
                        // check the detection score
                        // if it's above the threshold, draw it
                        // (the constant 50.0 is empirical: other cascades might require a different one)
                        if (dets[i][3] > 50.0) {
                            if (!faceDetected) {
                                faceDetected = true;
                                console.log("Face detected");
                                let imageData = document.getElementById('mycamera').toDataURL('jpg')
                                console.log(imageData);
                                take_snapshot(imageData)
                            }
                            var r, c, s;
                            //
                            ctx.beginPath();
                            ctx.arc(dets[i][1], dets[i][0], dets[i][2] / 2, 0, 2 * Math.PI, false);
                            ctx.lineWidth = 3;
                            ctx.strokeStyle = 'red';
                            ctx.stroke();
                            //
                            // find the eye pupils for each detected face
                            // starting regions for localization are initialized based on the face bounding box
                            // (parameters are set empirically)
                            // first eye
                            r = dets[i][0] - 0.075 * dets[i][2];
                            c = dets[i][1] - 0.175 * dets[i][2];
                            s = 0.35 * dets[i][2];
                            [r, c] = do_puploc(r, c, s, 63, image)
                            if (r >= 0 && c >= 0) {
                                ctx.beginPath();
                                ctx.arc(c, r, 1, 0, 2 * Math.PI, false);
                                ctx.lineWidth = 3;
                                ctx.strokeStyle = 'red';
                                ctx.stroke();
                            }
                            // second eye
                            r = dets[i][0] - 0.075 * dets[i][2];
                            c = dets[i][1] + 0.175 * dets[i][2];
                            s = 0.35 * dets[i][2];
                            [r, c] = do_puploc(r, c, s, 63, image)
                            if (r >= 0 && c >= 0) {
                                ctx.beginPath();
                                ctx.arc(c, r, 1, 0, 2 * Math.PI, false);
                                ctx.lineWidth = 3;
                                ctx.strokeStyle = 'red';
                                ctx.stroke();
                            }
                        }
                }
                /*
                    (5) instantiate camera handling (see https://github.com/cbrandolino/camvas)
                */
                var mycamvas = new camvas(ctx, processfn);
                /*
                    (6) it seems that everything went well
                */
                initialized = true;
            } else {
                alert("Waiting for socket to connect!");
            }
        }

        button_callback();

        function take_snapshot(base64Data) {
            // var d = new Date();
            // var milliseconds = d.getTime();
            // socket.emit('message', {
            //     "content": base64Data,
            //     "name": "kushal",
            //     "milliseconds": milliseconds
            // });
        }
        // socket.on("message", function (response) {
        //     try {
        //         console.log(response);
        //         const json = response.responseText;
        //         console.log(json);
        //         alert(json[0].status);
        //     } catch (e) {
        //         console.warn("Could not load!");
        //     }
        // });
    </script>
</body>

</html>